{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library imports and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "'''\n",
    "joints = [\n",
    "    \"(0, 0)\", \"(0, 1)\", \"(0, 2)\", \"(1, 0)\", \"(1, 1)\", \"(1, 2)\", \"(2, 0)\", \"(2, 1)\", \"(2, 2)\", \n",
    "    \"(3, 0)\", \"(3, 1)\", \"(3, 2)\", \"(4, 0)\", \"(4, 1)\", \"(4, 2)\", \"(5, 0)\", \"(5, 1)\", \"(5, 2)\", \n",
    "    \"(6, 0)\", \"(6, 1)\", \"(6, 2)\", \"(7, 0)\", \"(7, 1)\", \"(7, 2)\", \"(8, 0)\", \"(8, 1)\", \"(8, 2)\", \n",
    "    \"(9, 0)\", \"(9, 1)\", \"(9, 2)\", \"(10, 0)\", \"(10, 1)\", \"(10, 2)\", \"(11, 0)\", \"(11, 1)\", \"(11, 2)\", \n",
    "    \"(12, 0)\", \"(12, 1)\", \"(12, 2)\", \"(13, 0)\", \"(13, 1)\", \"(13, 2)\", \"(14, 0)\", \"(14, 1)\", \"(14, 2)\"\n",
    "]\n",
    "'''\n",
    "\n",
    "joints = [\n",
    "    \"(0, 0)\", \"(0, 1)\", \"(1, 0)\", \"(1, 1)\", \"(2, 0)\", \"(2, 1)\", \n",
    "    \"(3, 0)\", \"(3, 1)\", \"(4, 0)\", \"(4, 1)\", \"(5, 0)\", \"(5, 1)\", \n",
    "    \"(6, 0)\", \"(6, 1)\", \"(7, 0)\", \"(7, 1)\", \"(8, 0)\", \"(8, 1)\", \n",
    "    \"(9, 0)\", \"(9, 1)\", \"(10, 0)\", \"(10, 1)\", \"(11, 0)\", \"(11, 1)\", \n",
    "    \"(12, 0)\", \"(12, 1)\", \"(13, 0)\", \"(13, 1)\", \"(14, 0)\", \"(14, 1)\"\n",
    "]\n",
    "\n",
    "audio_col = [\n",
    "    'F0semitoneFrom27.5Hz_sma3nz_amean','F1amplitudeLogRelF0_sma3nz_amean','F1bandwidth_sma3nz_amean','F1frequency_sma3nz_amean',\n",
    "    'F2amplitudeLogRelF0_sma3nz_amean','F2bandwidth_sma3nz_amean','F2frequency_sma3nz_amean','F3amplitudeLogRelF0_sma3nz_amean',\n",
    "    'F3bandwidth_sma3nz_amean','F3frequency_sma3nz_amean','HNRdBACF_sma3nz_amean','alphaRatioV_sma3nz_amean',\n",
    "    'hammarbergIndexV_sma3nz_amean','jitterLocal_sma3nz_amean','logRelF0-H1-A3_sma3nz_amean','logRelF0-H1-H2_sma3nz_amean',\n",
    "    'loudness_sma3_amean','mfcc1_sma3_amean','mfcc2_sma3_amean','mfcc3_sma3_amean','mfcc4_sma3_amean','shimmerLocaldB_sma3nz_amean',\n",
    "    'slopeV0-500_sma3nz_amean','slopeV500-1500_sma3nz_amean','spectralFlux_sma3_amean'\n",
    "]\n",
    "\n",
    "# 12 DLD, 7 TD\n",
    "#DLD_child = ['DLD_bike_ban108.json','DLD_bike_chi108.json', 'DLD_bike_dou108.json', 'DLD_bike_fag108.json', 'DLD_bike_ken108.json', 'DLD_bike_kil108.json', 'DLD_bike_mcb108.json', 'DLD_bike_mel108.json', 'DLD_bike_peq108.json', 'DLD_bike_phi108.json', 'DLD_bike_ros108.json', 'DLD_bike_sul108.json']\n",
    "#TD_child = ['TD_bike_aik108.json', 'TD_bike_ham108.json', 'TD_bike_lav108.json', 'TD_bike_pem108.json', 'TD_bike_pia108.json', 'TD_bike_tro108.json', 'TD_bike_spo108.json']\n",
    "\n",
    "# no: gra, ben\n",
    "# yes: rob, son\n",
    "\n",
    "# 16 DLD, 11 TD\n",
    "# DLD_child = ['DLD_bike_mcg108.json','DLD_bike_bic108.json', 'DLD_bike_rob108.json', 'DLD_bike_son108.json', 'DLD_bike_ban108.json','DLD_bike_chi108.json', 'DLD_bike_dou108.json', 'DLD_bike_fag108.json', 'DLD_bike_ken108.json', 'DLD_bike_kil108.json', 'DLD_bike_mcb108.json', 'DLD_bike_mel108.json', 'DLD_bike_peq108.json', 'DLD_bike_phi108.json', 'DLD_bike_ros108.json', 'DLD_bike_sul108.json']\n",
    "# TD_child = ['TD_bike_ker108.json','TD_bike_cla108.json','TD_bike_ser108.json', 'TD_bike_mah108.json','TD_bike_aik108.json', 'TD_bike_ham108.json', 'TD_bike_lav108.json', 'TD_bike_pem108.json', 'TD_bike_pia108.json', 'TD_bike_tro108.json', 'TD_bike_spo108.json']\n",
    "\n",
    "DLD_child = ['mcg108.json','bic108.json', 'rob108.json', 'son108.json', 'ban108.json','chi108.json', 'dou108.json', 'fag108.json', 'ken108.json', 'kil108.json', 'mcb108.json', 'mel108.json', 'peq108.json', 'phi108.json', 'ros108.json', 'sul108.json']\n",
    "TD_child = ['ker108.json','cla108.json','ser108.json', 'mah108.json','aik108.json', 'ham108.json', 'lav108.json', 'pem108.json', 'pia108.json', 'tro108.json', 'spo108.json']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initalize fasttext api for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# select pretrained word vectorization \n",
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "wv = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# tokenizer to extract keywords  \n",
    "def spacy_tokenizer(sentence):\n",
    "\n",
    "    # get english words from sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "\n",
    "    # removing stop words\n",
    "    mytokens = [ word for word in mytokens if  word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# aggreate all words into single representation\n",
    "def sent_vec(sent):\n",
    "\n",
    "    # get vector size and create dummy holder\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "\n",
    "    # iterate through all words to add then average \n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/ctr\n",
    "\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    " this version of data preprocess will be tokenized and its temporal dimension will be removed by averaging all values within the same time stamp\n",
    "\n",
    " this will tokenization the two class: typically developing and late talkers (TD VS DLD)# code for all 3 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU GO BACK TO SINGLE EXPERIEMENT TEST, REMOVE THE [:-11]\n",
    "# if you want to combine dataset, remove the \"bike\" in filename part\n",
    "\n",
    "\n",
    "# set token size here:\n",
    "chunk_tokens = 50\n",
    "\n",
    "# *** THE FOLLOWING TOKENIZES THE TD & DLD DATA TO CREATE DATASET ***\n",
    "\n",
    "# chunk names = chunked person, duration  = duration of chunk\n",
    "#TD_Tokens_path = 'C:/USF/RESEARCH LABS/CSAIL/aphsia_baseline/joints_preprocessing/TD_normalized'\n",
    "TD_Tokens_path = 'C:/WORK/RESEARCH LABS/CSAIL/aphsia_baseline/dataset/TD_tokens_plus'\n",
    "chunk_names_TD = []\n",
    "duration_TD = []\n",
    "\n",
    "# create chunks for each feature type\n",
    "TD_chunk_audio = []\n",
    "TD_chunk_text = []\n",
    "TD_chunk_joints = []\n",
    "transcript_TD = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(TD_Tokens_path):\n",
    "\n",
    "    sub_transcript_TD = [] \n",
    "    # if not in TD skip\n",
    "    if filename[-11:] not in TD_child:\n",
    "        continue \n",
    "\n",
    "    if \"retell\" not in filename:\n",
    "        continue\n",
    "\n",
    "    # read file\n",
    "    child = filename[:-5]\n",
    "    file_path = os.path.join(TD_Tokens_path, filename)\n",
    "    df = pd.read_json(file_path)\n",
    "\n",
    "    # remove empty chunks and fill nans with 0s\n",
    "    df = df[df['chunk_id'].notna()]\n",
    "    df = df.fillna(0, inplace=False)\n",
    "\n",
    "    # create vector encodings of words\n",
    "    df['mod_token'] = df['token'].apply(spacy_tokenizer)\n",
    "    df['vec'] = df['mod_token'].apply(sent_vec)\n",
    "\n",
    "\n",
    "    # get number of rows \n",
    "    number_of_rows = df.shape[0]\n",
    "\n",
    "\n",
    "    # create start and end time stamps for duration calcuation\n",
    "    start = df['start'].to_numpy()\n",
    "    end = df['end'].to_numpy()\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        sub_transcript_TD.append(data['token'])\n",
    "\n",
    "\n",
    "\n",
    "    # create separate dfs for each features (audio, joints, text)\n",
    "    df_audio = df[audio_col]\n",
    "    df_joints = df[joints]\n",
    "    df_text = df['vec']\n",
    "    \n",
    "    audio_data = df_audio.to_numpy()\n",
    "    joint_data = df_joints.to_numpy()\n",
    "    text_data = df_text.to_list()\n",
    "\n",
    "\n",
    "    \n",
    "    # create lower and upper bounds for tokenization chunking\n",
    "    lower_bound = 0\n",
    "    upper_bound = 1\n",
    "    for i in range(number_of_rows//chunk_tokens):\n",
    "        # create lower and upper bounds for tokenization\n",
    "        lower = lower_bound * chunk_tokens\n",
    "        upper = (upper_bound * chunk_tokens)\n",
    "\n",
    "        # get range, list is exclusive of upper bound for all features \n",
    "        TD_chunk_audio.append(audio_data[lower:upper].tolist())\n",
    "        TD_chunk_joints.append(joint_data[lower:upper].tolist())\n",
    "        TD_chunk_text.append(text_data[lower:upper])\n",
    "\n",
    "        # get sub transcript per each token\n",
    "        transcript_TD.append(sub_transcript_TD[lower:upper])\n",
    "\n",
    "        chunk_names_TD.append(child)\n",
    "        duration_TD.append(end[upper -1 ] - start[lower])\n",
    "\n",
    "        # increment lower and upper bounds\n",
    "        lower_bound += 1\n",
    "        upper_bound += 1\n",
    "\n",
    "# chunk names = chunked person, duration  = duration of chunk\n",
    "#DLD_Tokens_path = 'C:/USF/RESEARCH LABS/CSAIL/aphsia_baseline/joints_preprocessing/DLD_normalized'\n",
    "DLD_Tokens_path = 'C:/WORK/RESEARCH LABS/CSAIL/aphsia_baseline/dataset/DLD_tokens_plus'\n",
    "chunk_names_DLD = []\n",
    "duration_DLD = []\n",
    "\n",
    "# create chunks for each feature type\n",
    "DLD_chunk_audio = []\n",
    "DLD_chunk_text = []\n",
    "DLD_chunk_joints = []\n",
    "transcript_DLD = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(DLD_Tokens_path):\n",
    "\n",
    "    sub_transcript_TD = [] \n",
    "    # if not in TD skip\n",
    "    if filename[-11:] not in DLD_child:\n",
    "        continue \n",
    "\n",
    "    if \"retell\" not in filename:\n",
    "        continue\n",
    "\n",
    "    # read file\n",
    "    child = filename[:-5]\n",
    "    file_path = os.path.join(DLD_Tokens_path, filename)\n",
    "    df = pd.read_json(file_path)\n",
    "\n",
    "    # remove empty chunks and fill nans with 0s\n",
    "    df = df[df['chunk_id'].notna()]\n",
    "    df = df.fillna(0, inplace=False)\n",
    "\n",
    "    # create vector encodings of words\n",
    "    df['mod_token'] = df['token'].apply(spacy_tokenizer)\n",
    "    df['vec'] = df['mod_token'].apply(sent_vec)\n",
    "\n",
    "\n",
    "    # get number of rows \n",
    "    number_of_rows = df.shape[0]\n",
    "\n",
    "\n",
    "    # create start and end time stamps for duration calcuation\n",
    "    start = df['start'].to_numpy()\n",
    "    end = df['end'].to_numpy()\n",
    "\n",
    "    for index, data in df.iterrows():\n",
    "        sub_transcript_TD.append(data['token'])\n",
    "\n",
    "\n",
    "\n",
    "    # create separate dfs for each features (audio, joints, text)\n",
    "    df_audio = df[audio_col]\n",
    "    df_joints = df[joints]\n",
    "    df_text = df['vec']\n",
    "    \n",
    "    audio_data = df_audio.to_numpy()\n",
    "    joint_data = df_joints.to_numpy()\n",
    "    text_data = df_text.to_list()\n",
    "\n",
    "\n",
    "    \n",
    "    # create lower and upper bounds for tokenization chunking\n",
    "    lower_bound = 0\n",
    "    upper_bound = 1\n",
    "    for i in range(number_of_rows//chunk_tokens):\n",
    "        # create lower and upper bounds for tokenization\n",
    "        lower = lower_bound * chunk_tokens\n",
    "        upper = (upper_bound * chunk_tokens)\n",
    "\n",
    "        # get range, list is exclusive of upper bound for all features \n",
    "        DLD_chunk_audio.append(audio_data[lower:upper].tolist())\n",
    "        DLD_chunk_joints.append(joint_data[lower:upper].tolist())\n",
    "        DLD_chunk_text.append(text_data[lower:upper])\n",
    "\n",
    "        # get sub transcript per each token\n",
    "        transcript_DLD.append(sub_transcript_TD[lower:upper])\n",
    "\n",
    "        chunk_names_DLD.append(child)\n",
    "        duration_DLD.append(end[upper -1 ] - start[lower])\n",
    "\n",
    "        # increment lower and upper bounds\n",
    "        lower_bound += 1\n",
    "        upper_bound += 1\n",
    "\n",
    "\n",
    "\n",
    "TD_audio = torch.tensor(TD_chunk_audio)\n",
    "TD_audio = TD_audio.mean(dim=1)\n",
    "DLD_audio = torch.tensor(DLD_chunk_audio)\n",
    "DLD_audio = DLD_audio.mean(dim=1)\n",
    "audio = torch.cat((TD_audio, DLD_audio), dim=0)\n",
    "\n",
    "TD_text = torch.tensor(TD_chunk_text)\n",
    "TD_text = TD_text.mean(dim=1)\n",
    "DLD_text = torch.tensor(DLD_chunk_text)\n",
    "DLD_text = DLD_text.mean(dim=1)\n",
    "text = torch.cat((TD_text, DLD_text), dim=0)\n",
    "\n",
    "TD_joints = torch.tensor(TD_chunk_joints)\n",
    "TD_joints = TD_joints.mean(dim=1)\n",
    "DLD_joints = torch.tensor(DLD_chunk_joints)\n",
    "DLD_joints = DLD_joints.mean(dim=1)\n",
    "joint = torch.cat((TD_joints, DLD_joints), dim=0)\n",
    "\n",
    "y_TD = torch.zeros(len(chunk_names_TD))\n",
    "y_DLD = torch.ones(len(chunk_names_DLD))\n",
    "y = torch.cat((y_TD, y_DLD), dim=0)\n",
    "\n",
    "y_names = chunk_names_TD + chunk_names_DLD\n",
    "transcript = transcript_TD + transcript_DLD\n",
    "y = [int(y) for y in y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save each modalities of data to npy and json files\n",
    "\n",
    "apply scaler to joint and visual (text is already scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "df_data = pd.DataFrame({\n",
    "    \"joint_feature_scaled\": scaler.fit_transform(joint).tolist(),\n",
    "    \"audio_feature_scaled\": scaler.fit_transform(audio).tolist(),\n",
    "    \"text_feature_scaled\": text.tolist(),  # Original features\n",
    "    \"transcript\": transcript,\n",
    "    \"label\": y,\n",
    "    \"name\": y_names\n",
    "})\n",
    "\n",
    "for index, data in df_data.iterrows():\n",
    "    full = \" \".join(data['transcript'])\n",
    "    df_data.at[index, 'full_transcript'] = full\n",
    "\n",
    "df_data['label'], df_data['full_transcript'] = df_data['full_transcript'], df_data['label']\n",
    "df_data.rename(columns={'label': 'full_transcript', 'full_transcript': 'label'}, inplace=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_json(\"data_50_retelll.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
